{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import lit, from_utc_timestamp, col, when, to_date, explode_outer, unbase64, from_json, from_unixtime, udf, trim as spark_trim\n",
    "from uuid import uuid4\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import StructType, ArrayType, StringType, NullType\n",
    "\n",
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json as json_lib\n",
    "import logging\n",
    "import re\n",
    "import time\n",
    "import notebookutils\n",
    "\n",
    "def trim(df):\n",
    "    stringCol = (c for c in df.schema if str(c.dataType) == \"StringType\")\n",
    "    for c in stringCol:\n",
    "        df = df.withColumn(c.name, spark_trim(col(c.name)))\n",
    "    return df\n",
    "\n",
    "def deDuplicate(df, subset=None):\n",
    "    df = df.dropDuplicates(subset)\n",
    "    return df\n",
    "\n",
    "def replaceNull(df, value, subset=None):\n",
    "    from datetime import datetime\n",
    "    target_type = None\n",
    "    \n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            datetime.strptime(value, \"%Y-%m-%d\")\n",
    "            target_type = \"date\"\n",
    "        except ValueError:\n",
    "            try:\n",
    "                datetime.strptime(value, \"%Y-%m-%dT%H:%M:%S\")\n",
    "                target_type = \"timestamp\"\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    columns_to_process = subset if subset is not None else df.columns\n",
    "\n",
    "    if target_type:\n",
    "        cols_to_fix = [\n",
    "            f.name for f in df.schema \n",
    "            if f.dataType.simpleString() == target_type \n",
    "            and f.nullable \n",
    "            and f.name in columns_to_process\n",
    "        ]\n",
    "        \n",
    "        update_exprs = [\n",
    "            when(col(c).isNull(), lit(value).cast(target_type)).otherwise(col(c)).alias(c) \n",
    "            if c in cols_to_fix else col(c) \n",
    "            for c in df.columns\n",
    "        ]\n",
    "        df = df.select(*update_exprs)\n",
    "    else:\n",
    "        df = df.fillna(value, subset=columns_to_process)\n",
    "    return df\n",
    "\n",
    "def drop_selected_columns(df, columns_to_drop):\n",
    "    return df.drop(*columns_to_drop)\n",
    "\n",
    "def use_selected_columns(df, columns_to_select):\n",
    "    return df.select(*columns_to_select)\n",
    "    \n",
    "def build_select_string_from_schema(df) -> str:\n",
    "    lines = [\n",
    "        f'col(\"{f.name}\").cast(\"{f.dataType.simpleString()}\").alias(\"{f.name}\")'\n",
    "        for f in df.schema.fields\n",
    "    ]\n",
    "    body = \",\\n\".join(lines)\n",
    "    print(f'''df = df.select(\\n{body}\\n)''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Python)",
   "language": "python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
