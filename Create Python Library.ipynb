{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "991c0148-648d-45e5-abdb-34fda20957d5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Your custom library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d3f374-d1d2-4392-b313-83d5479b067a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "code=r\"\"\"\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import lit, from_utc_timestamp, col, when, to_date, explode_outer, unbase64, from_json, from_unixtime, udf\n",
    "from uuid import uuid4\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import StructType, ArrayType, StringType, NullType\n",
    "\n",
    "import sempy.fabric as fabric\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json as json_lib\n",
    "import logging\n",
    "import re\n",
    "import time\n",
    "import notebookutils\n",
    "\n",
    "def trim(df):\n",
    "    stringCol= (col for col in df.schema if str(col.dataType)==\"StringType\")\n",
    "    for col in stringCol:\n",
    "        df = df.withColumn(col.name,trim(col.name))\n",
    "    return df\n",
    "  \n",
    "\n",
    "def deDuplicate(df, subset=None):\n",
    "    df = df.dropDuplicates(subset)\n",
    "    return df\n",
    "\n",
    "def replaceNull(df, value, subset=None):\n",
    "    from datetime import datetime\n",
    "    target_type = None\n",
    "    \n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            datetime.strptime(value, \"%Y-%m-%d\")\n",
    "            target_type = \"date\"\n",
    "        except ValueError:\n",
    "            try:\n",
    "                datetime.strptime(value, \"%Y-%m-%dT%H:%M:%S\")\n",
    "                target_type = \"timestamp\"\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    columns_to_process = subset if subset is not None else df.columns\n",
    "\n",
    "    if target_type:\n",
    "       \n",
    "        cols_to_fix = [\n",
    "            f.name for f in df.schema \n",
    "            if f.dataType.simpleString() == target_type \n",
    "            and f.nullable \n",
    "            and f.name in columns_to_process\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        update_exprs = [\n",
    "            when(col(c).isNull(), lit(value).cast(target_type)).otherwise(col(c)).alias(c) \n",
    "            if c in cols_to_fix else col(c) \n",
    "            for c in df.columns\n",
    "        ]\n",
    "        df = df.select(*update_exprs)\n",
    "    else:\n",
    "        df = df.fillna(value, subset=columns_to_process)\n",
    "\n",
    "    return df\n",
    "    \n",
    "    return df.fillna(value, subset)\n",
    "\n",
    "def drop_selected_columns(df, columns_to_drop):\n",
    "    return df.drop(*columns_to_drop)\n",
    "\n",
    "def use_selected_columns(df, columns_to_select):\n",
    "    return df.select(*columns_to_select)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77582bb-3e35-48c3-b19b-b364eaf66bc7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Save Library to lakehouse folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0194c40c-72dd-4bc5-9ba2-71315b90bbd9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "\n",
    "package_name = 'super_utils'\n",
    "target_path = 'abfs_path to lakehouse folder'\n",
    "\n",
    "code_content = textwrap.dedent(code).strip()\n",
    "os.makedirs(package_name, exist_ok=True)\n",
    "\n",
    "with open(f'{package_name}/functions.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(code_content)\n",
    "\n",
    "with open(f'{package_name}/__init__.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"from .functions import *\")\n",
    "\n",
    "setup_content = f\"\"\"\n",
    "from setuptools import setup, find_packages\n",
    "setup(\n",
    "    name='{package_name}',\n",
    "    version='0.1',\n",
    "    packages=find_packages(),\n",
    "    install_requires=['pyspark'],\n",
    ")\n",
    "\"\"\"\n",
    "with open('setup.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(setup_content)\n",
    "\n",
    "!python setup.py bdist_wheel\n",
    "\n",
    "wheel_file = [f for f in os.listdir('dist') if f.endswith('.whl')][0]\n",
    "local_wheel_path = f\"dist/{wheel_file}\"\n",
    "remote_wheel_path = f\"{target_path}/{wheel_file}\"\n",
    "\n",
    "# Copy to Lakehouse\n",
    "notebookutilsutils.fs.cp(f\"file://{os.getcwd()}/{local_wheel_path}\", remote_wheel_path)\n",
    "\n",
    "print(f\"Wheel saved to: {remote_wheel_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04f5f83-b540-487b-b286-c1f02f0d9777",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Load library to notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e960dee0-afb3-45e1-ad1e-5ba8820d8639",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_library():\n",
    "    import os\n",
    "    \n",
    "    path_abfss = target_path + filename\"\n",
    "    local_tmp = \"/tmp/filename\"\n",
    "    notebookutils.fs.cp(path_abfss, \"file:\" + local_tmp)\n",
    "    os.system(f\"pip install {local_tmp}\")\n",
    "\n",
    "get_library()\n",
    "\n",
    "import super_utils as su"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
